{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95622,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":80201,"modelId":104643}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install einops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom einops import rearrange\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\nimport os\nimport math\nimport h5py\nimport imageio\nimport warnings\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom itertools import repeat\nimport collections.abc\n\n# from interpretation_methods import *\n# from utils.imagenet_seg_loader import ImagenetSegLoader\n# from utils.model_loaders import vit_base_patch16_224_dino, vit_base_patch16_224\n# from utils.input_arguments import get_arg_parser\n# from utils.saver import Saver\n# from utils.sideplot import side_plot\n# from utils.image_denorm import image_vizformat\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:47:33.493232Z","iopub.execute_input":"2024-08-17T05:47:33.493972Z","iopub.status.idle":"2024-08-17T05:47:33.511352Z","shell.execute_reply.started":"2024-08-17T05:47:33.493940Z","shell.execute_reply":"2024-08-17T05:47:33.510546Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# <center><font size=\"+6\"> **1) Building the model**</font>","metadata":{}},{"cell_type":"markdown","source":"## **Config Setup**","metadata":{}},{"cell_type":"code","source":"def _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\nmodel_base_dir = \"/kaggle/input/popular-vits-for-interpretation/pytorch/default/1\"\ndefault_config = {\n    # patch models\n    'vit_small_patch16_224': _cfg(\n        url=f'{model_base_dir}/vit_small_p16_224-15ec54c9.pth',\n    ),\n    'vit_base_patch16_224': _cfg(\n        url=f'{model_base_dir}/jx_vit_base_p16_224-80ecf9dd.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_base_patch16_224_moco': _cfg(\n        url=f'{model_base_dir}/linear-vit-b-300ep.pth.tar',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_base_patch16_224_dino': _cfg(\n        url_backbone=f'{model_base_dir}/dino_vitbase16_pretrain.pth',\n        url_linear=f'{model_base_dir}/dino_vitbase16_linearweights.pth',\n        url=f'{model_base_dir}/dino_vitbase16_pretrain_full_checkpoint.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_mae_patch16_224': _cfg(\n        url=f'{model_base_dir}/mae_finetuned_vit_base.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_large_patch16_224': _cfg(\n        url=f'{model_base_dir}/jx_vit_large_p16_224-4ee7a4dc.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n}\n\ncls = ['airplane', 'bicycle', 'bird',\n       'boat',\n       'bottle',\n       'bus',\n       'car',\n       'cat',\n       'chair',\n       'cow',\n       'dining table',\n       'dog',\n       'horse',\n       'motobike',\n       'person',\n       'potted plant',\n       'sheep',\n       'sofa',\n       'train',\n       'tv'\n       ]","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:34:19.927534Z","iopub.execute_input":"2024-08-17T05:34:19.927947Z","iopub.status.idle":"2024-08-17T05:34:19.938577Z","shell.execute_reply.started":"2024-08-17T05:34:19.927903Z","shell.execute_reply":"2024-08-17T05:34:19.937678Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **Image Loader**","metadata":{}},{"cell_type":"code","source":"class ImagenetSegLoader(data.Dataset):\n    CLASSES = 2\n\n    def __init__(self, path, length_limit=None, transform=None, target_transform=None):\n        self.path = path\n        self.transform = transform\n        self.target_transform = target_transform\n        self.h5py = None\n        tmp = h5py.File(path, 'r')\n\n        if length_limit:\n            self.data_length = length_limit\n        else:\n            self.data_length = len(tmp['/value/img'])\n\n        tmp.close()\n        del tmp\n\n    def __getitem__(self, index):\n\n        if self.h5py is None:\n            self.h5py = h5py.File(self.path, 'r')\n\n        img = np.array(self.h5py[self.h5py['/value/img'][index, 0]]).transpose((2, 1, 0))\n        target = np.array(self.h5py[self.h5py[self.h5py['/value/gt'][index, 0]][0, 0]]).transpose((1, 0))\n\n        img = Image.fromarray(img).convert('RGB')\n        target = Image.fromarray(target)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = np.array(self.target_transform(target)).astype('int32')\n            target = torch.from_numpy(target).long()\n\n        return img, target\n\n    def __len__(self):\n        return self.data_length\n\n\ndef image_vizformat(image):\n    inv_normalize = transforms.Normalize(\n        mean=[-0.5 / 0.5, -0.5 / 0.5, -0.5 / 0.5],\n        std=[1 / 0.5, 1 / 0.5, 1 / 0.5]\n    )\n\n    img = inv_normalize(image[0])\n    img = torch.permute(img, (1, 2, 0))\n    img = img.detach().cpu().numpy()\n    return img\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Pretrained Models loader function**","metadata":{}},{"cell_type":"code","source":"import logging\nimport os\nimport math\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom typing import Callable\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n_logger = logging.getLogger(__name__)\n\n\"\"\"\n     Load pretrained model.\n     \n     For dino, the url_linear contain weight an bias for the head that needed to be injected into backbone. \n     Cause, the backbone doesn't have a classifier. Instead, is a very good feature extractor.\n     \n     No other model type require this processing.\n     \n\"\"\"\n\n\ndef load_pretrained(model, cfg=None, num_classes=1000, in_chans=3, filter_fn=None, strict=True, mae=False, moco=False, dino=False):\n    if cfg is None:\n        cfg = getattr(model, 'default_cfg')\n\n    if dino:\n        state_backbone = torch.load(cfg['url_backbone'], map_location='cpu')\n        state_linear = torch.load(cfg['url_linear'], map_location='cpu')[\n            'state_dict']  # Get weight of the last layer only.\n        state_dict = state_backbone.copy()\n        state_dict['head.weight'] = state_linear[\n            'module.linear.weight']  # Shape: (1000, 1536). That means, the final layer would have input from previous fc layer with 1536 neurons.\n        state_dict['head.bias'] = state_linear['module.linear.bias']  # Shape: (1000,)\n    else:\n        state_dict = torch.load(cfg['url'], map_location='cpu')\n\n    if mae:\n        state_dict = state_dict['model']\n    if moco:\n        state_dict = state_dict['state_dict']\n        for i in list(state_dict.keys()):\n            name = i.split('module.')[1]\n            state_dict[name] = state_dict.pop(i)\n\n    if filter_fn is not None:\n        state_dict = filter_fn(state_dict)\n\n    if in_chans == 1:\n        conv1_name = cfg['first_conv']\n        _logger.info('Converting first conv (%s) pretrained weights from 3 to 1 channel' % conv1_name)\n        conv1_weight = state_dict[conv1_name + '.weight']\n        # Some weights are in torch.half, ensure its float for sum on CPU\n        conv1_type = conv1_weight.dtype\n        conv1_weight = conv1_weight.float()\n        O, I, J, K = conv1_weight.shape\n        if I > 3:\n            assert conv1_weight.shape[1] % 3 == 0\n            conv1_weight = conv1_weight.reshape(O, I // 3, 3, J, K)  # For models with space2depth stems\n            conv1_weight = conv1_weight.sum(dim=2, keepdim=False)\n        else:\n            conv1_weight = conv1_weight.sum(dim=1, keepdim=True)\n        conv1_weight = conv1_weight.to(conv1_type)\n        state_dict[conv1_name + '.weight'] = conv1_weight\n    elif in_chans != 3:\n        conv1_name = cfg['first_conv']\n        conv1_weight = state_dict[conv1_name + '.weight']\n        conv1_type = conv1_weight.dtype\n        conv1_weight = conv1_weight.float()\n        O, I, J, K = conv1_weight.shape\n        if I != 3:\n            _logger.warning('Deleting first conv (%s) from pretrained weights.' % conv1_name)\n            del state_dict[conv1_name + '.weight']\n            strict = False\n        else:\n            # NOTE this strategy should be better than random init, but there could be other combinations of\n            # the original RGB input layer weights that'd work better for specific cases.\n            _logger.info('Repeating first conv (%s) weights in channel dim.' % conv1_name)\n            repeat = int(math.ceil(in_chans / 3))\n            conv1_weight = conv1_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]\n            conv1_weight *= (3 / float(in_chans))\n            conv1_weight = conv1_weight.to(conv1_type)\n            state_dict[conv1_name + '.weight'] = conv1_weight\n\n    classifier_name = cfg['classifier']\n    if num_classes == 1000 and cfg['num_classes'] == 1001:  # special case for imagenet trained models with extra background class in pretrained weights\n        classifier_weight = state_dict[classifier_name + '.weight']\n        state_dict[classifier_name + '.weight'] = classifier_weight[\n                                                  1:]  # Class index 0 give to background. Therefore, starting from index 1.\n        classifier_bias = state_dict[classifier_name + '.bias']\n        state_dict[classifier_name + '.bias'] = classifier_bias[1:]\n    elif num_classes != cfg['num_classes']:\n        # completely discard fully connected for all other differences between pretrained and created model\n        del state_dict[classifier_name + '.weight']\n        del state_dict[classifier_name + '.bias']\n        strict = False\n\n    model.load_state_dict(state_dict, strict=strict)\n\n\n\"\"\"\n    Load state dictionary of the pretrained model. \n\n\"\"\"\n\n\ndef load_state_dict(checkpoint_path, use_ema=False):\n    if checkpoint_path and os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        state_dict_key = 'state_dict'\n        if isinstance(checkpoint, dict):\n            if use_ema and 'state_dict_ema' in checkpoint:\n                state_dict_key = 'state_dict_ema'\n        if state_dict_key and state_dict_key in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[state_dict_key].items():\n                # strip `module.` prefix\n                name = k[7:] if k.startswith('module') else k\n                new_state_dict[name] = v\n            state_dict = new_state_dict\n        else:\n            state_dict = checkpoint\n        _logger.info(\"Loaded {} from checkpoint '{}'\".format(state_dict_key, checkpoint_path))\n        return state_dict\n    else:\n        _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n        raise FileNotFoundError()\n\n        \n  \n\"\"\"\n\n    @@@@ From model_loaders.py\n\n\"\"\"\n\ndef _conv_filter(state_dict, patch_size=16):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    out_dict = {}\n    for k, v in state_dict.items():\n        if 'patch_embed.proj.weight' in k:\n            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n        out_dict[k] = v\n    return out_dict\n\n\ndef vit_small_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=8, num_heads=6, mlp_ratio=3, qkv_bias=False,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=8, num_heads=6, mlp_ratio=3, qkv_bias=False,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\n    model.default_cfg = default_config['vit_small_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n    return model\n\n\ndef vit_base_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = default_config['vit_base_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n    return model\n\n\ndef vit_base_patch16_224_dino(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      dino=True, **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), dino=True, **kwargs)\n\n    model.default_cfg = default_config['vit_base_patch16_224_dino']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter, dino=True)\n    return model\n\n\ndef vit_base_patch16_224_moco(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\n    model.default_cfg = default_config['vit_base_patch16_224_moco']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter, moco=True)\n    return model\n\n\ndef vit_mae_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      mae=True, **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), global_pooling=True, **kwargs)\n    model.default_cfg = default_config['vit_mae_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter, mae=True)\n    return model\n\n\ndef vit_large_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = default_config['vit_large_patch16_224']\n    if pretrained:\n        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:38:03.810112Z","iopub.execute_input":"2024-08-17T05:38:03.810590Z","iopub.status.idle":"2024-08-17T05:38:04.021170Z","shell.execute_reply.started":"2024-08-17T05:38:03.810555Z","shell.execute_reply":"2024-08-17T05:38:04.020174Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## **Input arguments - input_arguments.py**","metadata":{}},{"cell_type":"code","source":"import argparse\n\n\ndef get_arg_parser():\n    parser = argparse.ArgumentParser(description='Training multi-class classifier')\n    parser.add_argument('--arc', type=str, default='vgg', metavar='N',\n                        help='Model architecture')\n    parser.add_argument('--train_dataset', type=str, default='imagenet', metavar='N',\n                        help='Testing Dataset')\n    parser.add_argument('--method', type=str,\n                        default='ours_c',\n                        choices=['rollout', 'lrp', 'partial_lrp', 'transformer_attribution', 'attn_last_layer',\n                                 'attn_gradcam', 'generic_attribution', 'ours', 'ours_c'],\n                        help='')\n    parser.add_argument('--thr', type=float, default=0.,\n                        help='threshold')\n    parser.add_argument('--start_layer', type=int, default=4,\n                        help='start_layer')\n    parser.add_argument('--K', type=int, default=1,\n                        help='new - top K results')\n    parser.add_argument('--save-img', action='store_true',\n                        default=True,\n                        help='')\n    parser.add_argument('--no-ia', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-fx', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-fgx', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-m', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-reg', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--is-ablation', type=bool,\n                        default=False,\n                        help='')\n    parser.add_argument('--len-lim', type=int,\n                        default=1,\n                        help='')\n    parser.add_argument('--imagenet-seg-path', type=str, required=True)\n\n    return parser\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:38:35.354386Z","iopub.execute_input":"2024-08-17T05:38:35.354734Z","iopub.status.idle":"2024-08-17T05:38:35.365644Z","shell.execute_reply.started":"2024-08-17T05:38:35.354706Z","shell.execute_reply":"2024-08-17T05:38:35.364717Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\n\n\ndef image_vizformat(img):\n    inr = transforms.Normalize(mean=[-0.5/.5, -0.5/.5, -0.5/.5], std=[1/0.5, 1/0.5, 1/0.5])\n    img = inr(img[0])\n    img = torch.permute(img, (1, 2, 0))\n    return img.detach().cpu().numpy()\n\ndef plot(image, isImage=True):\n    if isImage:\n        image = image_vizformat(image)\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:50:36.965600Z","iopub.execute_input":"2024-08-17T05:50:36.966639Z","iopub.status.idle":"2024-08-17T05:50:36.975663Z","shell.execute_reply.started":"2024-08-17T05:50:36.966603Z","shell.execute_reply":"2024-08-17T05:50:36.974814Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# <font color=\"#DC143C\"> **Building the Vision Transformer model**","metadata":{}},{"cell_type":"markdown","source":"## **Initializing the weight**","metadata":{}},{"cell_type":"code","source":"def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n                      \"The distribution of values may be incorrect.\",\n                      stacklevel=2)\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:41:03.918151Z","iopub.execute_input":"2024-08-17T05:41:03.918476Z","iopub.status.idle":"2024-08-17T05:41:03.928205Z","shell.execute_reply.started":"2024-08-17T05:41:03.918450Z","shell.execute_reply":"2024-08-17T05:41:03.927294Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## **MLP**","metadata":{}},{"cell_type":"code","source":"class Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Attention**","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5  # This is root_over(dh) with which we divided Q.KT\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)  # Attention's multiplication. That is, we have to pass the attention output through a Linear layer. That is the linear layer.\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.attn_gradients = None\n        self.attention_map = None\n\n        self.vproj = None\n        self.input = None\n\n    def save_input(self, z):\n        self.input = z\n\n    def get_input(self):\n        return self.input\n\n    # Backward hook. Therefore, param is automatically gradients for that layer.\n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n\n    def get_attn_gradients(self):\n        return self.attn_gradients\n\n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n\n    def get_attention_map(self):\n        return self.attention_map\n\n    def forward(self, x, register_hook=False):\n        b, n, _, h = *x.shape, self.num_heads\n        self.save_input(x)\n\n        qkv = self.qkv(x)\n        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n\n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n\n        attn = dots.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        out = torch.einsum('bhij,bhjd->bhid', attn, v)  # Simple torch.matmul actually.\n\n        self.save_attention_map(attn)\n        if register_hook:\n            attn.register_hook(self.save_attn_gradients)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n\n        out = self.proj(out)\n        out = self.proj_drop(out)\n\n        self.vproj = torch.matmul(rearrange(v, 'b h n d -> b n (h d)'), self.proj.weight.t())\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:45:39.929376Z","iopub.execute_input":"2024-08-17T05:45:39.930167Z","iopub.status.idle":"2024-08-17T05:45:39.943942Z","shell.execute_reply.started":"2024-08-17T05:45:39.930128Z","shell.execute_reply":"2024-08-17T05:45:39.943031Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## **Encoder block**","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n        self.norm2 = norm_layer(dim)\n\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)  # hidden_features is the output layer's neuron count.\n\n        self.input = None\n        self.output = None\n        self.tilde = None\n\n    def save_input(self, z):\n        self.input = z\n\n    def get_input(self):\n        return self.input\n\n    def save_tilde(self, z):\n        self.tilde = z\n\n    def get_tilde(self):\n        return self.tilde\n\n    def save_output(self, z):\n        self.output = z\n\n    def get_output(self):\n        return self.output\n\n    def forward(self, x, register_hook=False):\n        self.save_input(x)\n        out = x + self.attn(self.norm1(x), register_hook=register_hook)\n        self.save_tilde(out)\n        out = out + self.mlp(self.norm2(out))\n        self.save_output(out)\n\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Patch Embeddings**","metadata":{}},{"cell_type":"code","source":"class PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#A02334\"> **Vision Transformer Model** </font>","metadata":{}},{"cell_type":"code","source":"\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, drop_rate=0., attn_drop_rate=0., norm_layer=nn.LayerNorm,\n                 global_pooling=False, dino=False):\n        super().__init__()\n        self.global_pooling = global_pooling\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, norm_layer=norm_layer)\n            for i in range(depth)])\n        if global_pooling:\n            self.fc_norm = norm_layer(embed_dim)\n        else:\n            self.norm = norm_layer(embed_dim)\n\n        # Classifier head\n        if dino:\n            self.head = nn.Linear(embed_dim * 2, num_classes) if num_classes > 0 else nn.Identity()\n        else:\n            self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        self.cls_gradients = None\n        self.input_grad = None\n        self.dino = dino\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def save_cls_gradients(self, cls_gradients):\n        self.cls_gradients = cls_gradients\n\n    def get_cls_gradients(self):\n        return self.cls_gradients\n\n    def save_input_gradients(self, input_grad):\n        self.input_grad = input_grad\n\n    def get_input_gradients(self):\n        return self.input_grad\n\n    def forward(self, x, register_hook=False):\n        B = x.shape[0]\n        #         if register_hook:\n        #             x.register_hook(self.save_input_gradients)\n\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.blocks[:-1]:\n            x = blk(x, register_hook=register_hook)\n\n        x.register_hook(self.save_cls_gradients)\n\n        x = self.blocks[-1](x, register_hook=register_hook)\n\n        if self.global_pooling:\n            x = x[:, 1:, :].mean(axis=1)\n            x = self.fc_norm(x)\n        elif self.dino:\n            x = self.norm(x)\n            x = torch.cat((x[:, 0].unsqueeze(-1), x[:, 1:, :].mean(axis=1).unsqueeze(-1)), dim=-1)\n            x = x.reshape(x.shape[0], -1)\n        else:\n            x = self.norm(x)\n            x = x[:, 0]\n\n        x = self.head(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center><font size=\"+6\"> **Run and Test** </font>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}