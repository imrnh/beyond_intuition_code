{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95622,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":80201,"modelId":104643}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install einops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom einops import rearrange\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\nimport math\nimport h5py\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport imageio, warnings, os\n\n# from interpretation_methods import *\n# from utils.imagenet_seg_loader import ImagenetSegLoader\n# from utils.model_loaders import vit_base_patch16_224_dino, vit_base_patch16_224\n# from utils.input_arguments import get_arg_parser\n# from utils.saver import Saver\n# from utils.sideplot import side_plot\n# from utils.image_denorm import image_vizformat\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center><font size=\"+6\"> **1) Building the model**</font>","metadata":{}},{"cell_type":"markdown","source":"## **Config Setup**","metadata":{}},{"cell_type":"code","source":"def _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\nmodel_base_dir = \"/kaggle/input/popular-vits-for-interpretation/pytorch/default/1\"\ndefault_config = {\n    # patch models\n    'vit_small_patch16_224': _cfg(\n        url=f'{model_base_dir}/vit_small_p16_224-15ec54c9.pth',\n    ),\n    'vit_base_patch16_224': _cfg(\n        url=f'{model_base_dir}/jx_vit_base_p16_224-80ecf9dd.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_base_patch16_224_moco': _cfg(\n        url=f'{model_base_dir}/linear-vit-b-300ep.pth.tar',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_base_patch16_224_dino': _cfg(\n        url_backbone=f'{model_base_dir}/dino_vitbase16_pretrain.pth',\n        url_linear=f'{model_base_dir}/dino_vitbase16_linearweights.pth',\n        url=f'{model_base_dir}/dino_vitbase16_pretrain_full_checkpoint.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_mae_patch16_224': _cfg(\n        url=f'{model_base_dir}/mae_finetuned_vit_base.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_large_patch16_224': _cfg(\n        url=f'{model_base_dir}/jx_vit_large_p16_224-4ee7a4dc.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n}\n\ncls = ['airplane', 'bicycle', 'bird',\n       'boat',\n       'bottle',\n       'bus',\n       'car',\n       'cat',\n       'chair',\n       'cow',\n       'dining table',\n       'dog',\n       'horse',\n       'motobike',\n       'person',\n       'potted plant',\n       'sheep',\n       'sofa',\n       'train',\n       'tv'\n       ]","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:34:19.927534Z","iopub.execute_input":"2024-08-17T05:34:19.927947Z","iopub.status.idle":"2024-08-17T05:34:19.938577Z","shell.execute_reply.started":"2024-08-17T05:34:19.927903Z","shell.execute_reply":"2024-08-17T05:34:19.937678Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **Image Loader**","metadata":{}},{"cell_type":"code","source":"class ImagenetSegLoader(data.Dataset):\n    CLASSES = 2\n\n    def __init__(self, path, length_limit=None, transform=None, target_transform=None):\n        self.path = path\n        self.transform = transform\n        self.target_transform = target_transform\n        self.h5py = None\n        tmp = h5py.File(path, 'r')\n\n        if length_limit:\n            self.data_length = length_limit\n        else:\n            self.data_length = len(tmp['/value/img'])\n\n        tmp.close()\n        del tmp\n\n    def __getitem__(self, index):\n\n        if self.h5py is None:\n            self.h5py = h5py.File(self.path, 'r')\n\n        img = np.array(self.h5py[self.h5py['/value/img'][index, 0]]).transpose((2, 1, 0))\n        target = np.array(self.h5py[self.h5py[self.h5py['/value/gt'][index, 0]][0, 0]]).transpose((1, 0))\n\n        img = Image.fromarray(img).convert('RGB')\n        target = Image.fromarray(target)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = np.array(self.target_transform(target)).astype('int32')\n            target = torch.from_numpy(target).long()\n\n        return img, target\n\n    def __len__(self):\n        return self.data_length\n\n\ndef image_vizformat(image):\n    inv_normalize = transforms.Normalize(\n        mean=[-0.5 / 0.5, -0.5 / 0.5, -0.5 / 0.5],\n        std=[1 / 0.5, 1 / 0.5, 1 / 0.5]\n    )\n\n    img = inv_normalize(image[0])\n    img = torch.permute(img, (1, 2, 0))\n    img = img.detach().cpu().numpy()\n    return img\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Pretrained Models loader function**","metadata":{}},{"cell_type":"code","source":"import logging\nimport os\nimport math\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom typing import Callable\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n_logger = logging.getLogger(__name__)\n\n\"\"\"\n     Load pretrained model.\n     \n     For dino, the url_linear contain weight an bias for the head that needed to be injected into backbone. \n     Cause, the backbone doesn't have a classifier. Instead, is a very good feature extractor.\n     \n     No other model type require this processing.\n     \n\"\"\"\n\n\ndef load_pretrained(model, cfg=None, num_classes=1000, in_chans=3, filter_fn=None, strict=True, mae=False, moco=False, dino=False):\n    if cfg is None:\n        cfg = getattr(model, 'default_cfg')\n\n    if dino:\n        state_backbone = torch.load(cfg['url_backbone'], map_location='cpu')\n        state_linear = torch.load(cfg['url_linear'], map_location='cpu')[\n            'state_dict']  # Get weight of the last layer only.\n        state_dict = state_backbone.copy()\n        state_dict['head.weight'] = state_linear[\n            'module.linear.weight']  # Shape: (1000, 1536). That means, the final layer would have input from previous fc layer with 1536 neurons.\n        state_dict['head.bias'] = state_linear['module.linear.bias']  # Shape: (1000,)\n    else:\n        state_dict = torch.load(cfg['url'], map_location='cpu')\n\n    if mae:\n        state_dict = state_dict['model']\n    if moco:\n        state_dict = state_dict['state_dict']\n        for i in list(state_dict.keys()):\n            name = i.split('module.')[1]\n            state_dict[name] = state_dict.pop(i)\n\n    if filter_fn is not None:\n        state_dict = filter_fn(state_dict)\n\n    if in_chans == 1:\n        conv1_name = cfg['first_conv']\n        _logger.info('Converting first conv (%s) pretrained weights from 3 to 1 channel' % conv1_name)\n        conv1_weight = state_dict[conv1_name + '.weight']\n        # Some weights are in torch.half, ensure its float for sum on CPU\n        conv1_type = conv1_weight.dtype\n        conv1_weight = conv1_weight.float()\n        O, I, J, K = conv1_weight.shape\n        if I > 3:\n            assert conv1_weight.shape[1] % 3 == 0\n            conv1_weight = conv1_weight.reshape(O, I // 3, 3, J, K)  # For models with space2depth stems\n            conv1_weight = conv1_weight.sum(dim=2, keepdim=False)\n        else:\n            conv1_weight = conv1_weight.sum(dim=1, keepdim=True)\n        conv1_weight = conv1_weight.to(conv1_type)\n        state_dict[conv1_name + '.weight'] = conv1_weight\n    elif in_chans != 3:\n        conv1_name = cfg['first_conv']\n        conv1_weight = state_dict[conv1_name + '.weight']\n        conv1_type = conv1_weight.dtype\n        conv1_weight = conv1_weight.float()\n        O, I, J, K = conv1_weight.shape\n        if I != 3:\n            _logger.warning('Deleting first conv (%s) from pretrained weights.' % conv1_name)\n            del state_dict[conv1_name + '.weight']\n            strict = False\n        else:\n            # NOTE this strategy should be better than random init, but there could be other combinations of\n            # the original RGB input layer weights that'd work better for specific cases.\n            _logger.info('Repeating first conv (%s) weights in channel dim.' % conv1_name)\n            repeat = int(math.ceil(in_chans / 3))\n            conv1_weight = conv1_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]\n            conv1_weight *= (3 / float(in_chans))\n            conv1_weight = conv1_weight.to(conv1_type)\n            state_dict[conv1_name + '.weight'] = conv1_weight\n\n    classifier_name = cfg['classifier']\n    if num_classes == 1000 and cfg['num_classes'] == 1001:  # special case for imagenet trained models with extra background class in pretrained weights\n        classifier_weight = state_dict[classifier_name + '.weight']\n        state_dict[classifier_name + '.weight'] = classifier_weight[\n                                                  1:]  # Class index 0 give to background. Therefore, starting from index 1.\n        classifier_bias = state_dict[classifier_name + '.bias']\n        state_dict[classifier_name + '.bias'] = classifier_bias[1:]\n    elif num_classes != cfg['num_classes']:\n        # completely discard fully connected for all other differences between pretrained and created model\n        del state_dict[classifier_name + '.weight']\n        del state_dict[classifier_name + '.bias']\n        strict = False\n\n    model.load_state_dict(state_dict, strict=strict)\n\n\n\"\"\"\n    Load state dictionary of the pretrained model. \n\n\"\"\"\n\n\ndef load_state_dict(checkpoint_path, use_ema=False):\n    if checkpoint_path and os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        state_dict_key = 'state_dict'\n        if isinstance(checkpoint, dict):\n            if use_ema and 'state_dict_ema' in checkpoint:\n                state_dict_key = 'state_dict_ema'\n        if state_dict_key and state_dict_key in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[state_dict_key].items():\n                # strip `module.` prefix\n                name = k[7:] if k.startswith('module') else k\n                new_state_dict[name] = v\n            state_dict = new_state_dict\n        else:\n            state_dict = checkpoint\n        _logger.info(\"Loaded {} from checkpoint '{}'\".format(state_dict_key, checkpoint_path))\n        return state_dict\n    else:\n        _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n        raise FileNotFoundError()\n\n        \n  \n\"\"\"\n\n    @@@@ From model_loaders.py\n\n\"\"\"\n\ndef _conv_filter(state_dict, patch_size=16):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    out_dict = {}\n    for k, v in state_dict.items():\n        if 'patch_embed.proj.weight' in k:\n            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n        out_dict[k] = v\n    return out_dict\n\n\ndef vit_small_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=8, num_heads=6, mlp_ratio=3, qkv_bias=False,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=8, num_heads=6, mlp_ratio=3, qkv_bias=False,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\n    model.default_cfg = default_config['vit_small_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n    return model\n\n\ndef vit_base_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = default_config['vit_base_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n    return model\n\n\ndef vit_base_patch16_224_dino(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      dino=True, **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), dino=True, **kwargs)\n\n    model.default_cfg = default_config['vit_base_patch16_224_dino']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter, dino=True)\n    return model\n\n\ndef vit_base_patch16_224_moco(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\n    model.default_cfg = default_config['vit_base_patch16_224_moco']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter, moco=True)\n    return model\n\n\ndef vit_mae_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      mae=True, **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), global_pooling=True, **kwargs)\n    model.default_cfg = default_config['vit_mae_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter, mae=True)\n    return model\n\n\ndef vit_large_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = default_config['vit_large_patch16_224']\n    if pretrained:\n        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:38:03.810112Z","iopub.execute_input":"2024-08-17T05:38:03.810590Z","iopub.status.idle":"2024-08-17T05:38:04.021170Z","shell.execute_reply.started":"2024-08-17T05:38:03.810555Z","shell.execute_reply":"2024-08-17T05:38:04.020174Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## **Input arguments - input_arguments.py**","metadata":{}},{"cell_type":"code","source":"import argparse\n\n\ndef get_arg_parser():\n    parser = argparse.ArgumentParser(description='Training multi-class classifier')\n    parser.add_argument('--arc', type=str, default='vgg', metavar='N',\n                        help='Model architecture')\n    parser.add_argument('--train_dataset', type=str, default='imagenet', metavar='N',\n                        help='Testing Dataset')\n    parser.add_argument('--method', type=str,\n                        default='ours_c',\n                        choices=['rollout', 'lrp', 'partial_lrp', 'transformer_attribution', 'attn_last_layer',\n                                 'attn_gradcam', 'generic_attribution', 'ours', 'ours_c'],\n                        help='')\n    parser.add_argument('--thr', type=float, default=0.,\n                        help='threshold')\n    parser.add_argument('--start_layer', type=int, default=4,\n                        help='start_layer')\n    parser.add_argument('--K', type=int, default=1,\n                        help='new - top K results')\n    parser.add_argument('--save-img', action='store_true',\n                        default=True,\n                        help='')\n    parser.add_argument('--no-ia', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-fx', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-fgx', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-m', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-reg', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--is-ablation', type=bool,\n                        default=False,\n                        help='')\n    parser.add_argument('--len-lim', type=int,\n                        default=1,\n                        help='')\n    parser.add_argument('--imagenet-seg-path', type=str, required=True)\n\n    return parser\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:38:35.354386Z","iopub.execute_input":"2024-08-17T05:38:35.354734Z","iopub.status.idle":"2024-08-17T05:38:35.365644Z","shell.execute_reply.started":"2024-08-17T05:38:35.354706Z","shell.execute_reply":"2024-08-17T05:38:35.364717Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# <font color=\"#DC143C\"> **Building the Vision Transformer model**","metadata":{}},{"cell_type":"markdown","source":"## **Initializing the weight**","metadata":{}},{"cell_type":"code","source":"def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n                      \"The distribution of values may be incorrect.\",\n                      stacklevel=2)\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T05:41:03.918151Z","iopub.execute_input":"2024-08-17T05:41:03.918476Z","iopub.status.idle":"2024-08-17T05:41:03.928205Z","shell.execute_reply.started":"2024-08-17T05:41:03.918450Z","shell.execute_reply":"2024-08-17T05:41:03.927294Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## **MLP**","metadata":{}},{"cell_type":"code","source":"class Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Attention**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}