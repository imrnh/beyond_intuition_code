{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9189488,"sourceType":"datasetVersion","datasetId":5555019},{"sourceId":95622,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":80201,"modelId":104643}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Installation and Imports**","metadata":{}},{"cell_type":"code","source":"!pip3 install einops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom copy import deepcopy\nfrom typing import Callable\nfrom itertools import repeat\nfrom functools import partial\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nfrom IPython.display import clear_output\nimport torch.utils.model_zoo as model_zoo\nimport os, math,h5py, imageio, warnings, logging, collections.abc, cv2\n\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nimport torch.nn.functional as F\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\nwarnings.filterwarnings(\"ignore\")\n_logger = logging.getLogger(__name__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center><font size=\"+6\"> **1) Building the model**</font>","metadata":{}},{"cell_type":"markdown","source":"## **Config Setup**","metadata":{}},{"cell_type":"code","source":"def _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\nmodel_base_dir = \"/kaggle/input/popular-vits-for-interpretation/pytorch/default/1\"\ndefault_config = {\n    # patch models\n    'vit_small_patch16_224': _cfg(\n        url=f'{model_base_dir}/vit_small_p16_224-15ec54c9.pth',\n    ),\n    'vit_base_patch16_224': _cfg(\n        url=f'{model_base_dir}/jx_vit_base_p16_224-80ecf9dd.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_base_patch16_224_moco': _cfg(\n        url=f'{model_base_dir}/linear-vit-b-300ep.pth.tar',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_base_patch16_224_dino': _cfg(\n        url_backbone=f'{model_base_dir}/dino_vitbase16_pretrain.pth',\n        url_linear=f'{model_base_dir}/dino_vitbase16_linearweights.pth',\n        url=f'{model_base_dir}/dino_vitbase16_pretrain_full_checkpoint.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_mae_patch16_224': _cfg(\n        url=f'{model_base_dir}/mae_finetuned_vit_base.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'vit_large_patch16_224': _cfg(\n        url=f'{model_base_dir}/jx_vit_large_p16_224-4ee7a4dc.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n}\n\ncls = ['airplane', 'bicycle', 'bird',\n       'boat',\n       'bottle',\n       'bus',\n       'car',\n       'cat',\n       'chair',\n       'cow',\n       'dining table',\n       'dog',\n       'horse',\n       'motobike',\n       'person',\n       'potted plant',\n       'sheep',\n       'sofa',\n       'train',\n       'tv'\n       ]","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Image Loader**","metadata":{}},{"cell_type":"code","source":"class ImagenetSegLoader(data.Dataset):\n    CLASSES = 2\n\n    def __init__(self, path, length_limit=None, transform=None, target_transform=None):\n        self.path = path\n        self.transform = transform\n        self.target_transform = target_transform\n        self.h5py = None\n        tmp = h5py.File(path, 'r')\n\n        if length_limit:\n            self.data_length = length_limit\n        else:\n            self.data_length = len(tmp['/value/img'])\n\n        tmp.close()\n        del tmp\n\n    def __getitem__(self, index):\n\n        if self.h5py is None:\n            self.h5py = h5py.File(self.path, 'r')\n\n        img = np.array(self.h5py[self.h5py['/value/img'][index, 0]]).transpose((2, 1, 0))\n        target = np.array(self.h5py[self.h5py[self.h5py['/value/gt'][index, 0]][0, 0]]).transpose((1, 0))\n\n        img = Image.fromarray(img).convert('RGB')\n        target = Image.fromarray(target)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = np.array(self.target_transform(target)).astype('int32')\n            target = torch.from_numpy(target).long()\n\n        return img, target\n\n    def __len__(self):\n        return self.data_length\n\n\ndef image_vizformat(image):\n    inv_normalize = transforms.Normalize(\n        mean=[-0.5 / 0.5, -0.5 / 0.5, -0.5 / 0.5],\n        std=[1 / 0.5, 1 / 0.5, 1 / 0.5]\n    )\n\n    img = inv_normalize(image[0])\n    img = torch.permute(img, (1, 2, 0))\n    img = img.detach().cpu().numpy()\n    return img\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Pretrained Models loader function**","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    \n    @@ From load_pretrained.py\n\n    Load pretrained model.\n     \n     For dino, the url_linear contain weight an bias for the head that needed to be injected into backbone. \n     Cause, the backbone doesn't have a classifier. Instead, is a very good feature extractor.\n     \n     No other model type require this processing.\n     \n\"\"\"\n\n\ndef load_pretrained(model, cfg=None, num_classes=1000, in_chans=3, filter_fn=None, strict=True, mae=False, moco=False, dino=False):\n    if cfg is None:\n        cfg = getattr(model, 'default_cfg')\n\n    if dino:\n        state_backbone = torch.load(cfg['url_backbone'], map_location='cpu')\n        state_linear = torch.load(cfg['url_linear'], map_location='cpu')[\n            'state_dict']  # Get weight of the last layer only.\n        state_dict = state_backbone.copy()\n        state_dict['head.weight'] = state_linear[\n            'module.linear.weight']  # Shape: (1000, 1536). That means, the final layer would have input from previous fc layer with 1536 neurons.\n        state_dict['head.bias'] = state_linear['module.linear.bias']  # Shape: (1000,)\n    else:\n        state_dict = torch.load(cfg['url'], map_location='cpu')\n\n    if mae:\n        state_dict = state_dict['model']\n    if moco:\n        state_dict = state_dict['state_dict']\n        for i in list(state_dict.keys()):\n            name = i.split('module.')[1]\n            state_dict[name] = state_dict.pop(i)\n\n    if filter_fn is not None:\n        state_dict = filter_fn(state_dict)\n\n    if in_chans == 1:\n        conv1_name = cfg['first_conv']\n        _logger.info('Converting first conv (%s) pretrained weights from 3 to 1 channel' % conv1_name)\n        conv1_weight = state_dict[conv1_name + '.weight']\n        # Some weights are in torch.half, ensure its float for sum on CPU\n        conv1_type = conv1_weight.dtype\n        conv1_weight = conv1_weight.float()\n        O, I, J, K = conv1_weight.shape\n        if I > 3:\n            assert conv1_weight.shape[1] % 3 == 0\n            conv1_weight = conv1_weight.reshape(O, I // 3, 3, J, K)  # For models with space2depth stems\n            conv1_weight = conv1_weight.sum(dim=2, keepdim=False)\n        else:\n            conv1_weight = conv1_weight.sum(dim=1, keepdim=True)\n        conv1_weight = conv1_weight.to(conv1_type)\n        state_dict[conv1_name + '.weight'] = conv1_weight\n    elif in_chans != 3:\n        conv1_name = cfg['first_conv']\n        conv1_weight = state_dict[conv1_name + '.weight']\n        conv1_type = conv1_weight.dtype\n        conv1_weight = conv1_weight.float()\n        O, I, J, K = conv1_weight.shape\n        if I != 3:\n            _logger.warning('Deleting first conv (%s) from pretrained weights.' % conv1_name)\n            del state_dict[conv1_name + '.weight']\n            strict = False\n        else:\n            # NOTE this strategy should be better than random init, but there could be other combinations of\n            # the original RGB input layer weights that'd work better for specific cases.\n            _logger.info('Repeating first conv (%s) weights in channel dim.' % conv1_name)\n            repeat = int(math.ceil(in_chans / 3))\n            conv1_weight = conv1_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]\n            conv1_weight *= (3 / float(in_chans))\n            conv1_weight = conv1_weight.to(conv1_type)\n            state_dict[conv1_name + '.weight'] = conv1_weight\n\n    classifier_name = cfg['classifier']\n    if num_classes == 1000 and cfg['num_classes'] == 1001:  # special case for imagenet trained models with extra background class in pretrained weights\n        classifier_weight = state_dict[classifier_name + '.weight']\n        state_dict[classifier_name + '.weight'] = classifier_weight[\n                                                  1:]  # Class index 0 give to background. Therefore, starting from index 1.\n        classifier_bias = state_dict[classifier_name + '.bias']\n        state_dict[classifier_name + '.bias'] = classifier_bias[1:]\n    elif num_classes != cfg['num_classes']:\n        # completely discard fully connected for all other differences between pretrained and created model\n        del state_dict[classifier_name + '.weight']\n        del state_dict[classifier_name + '.bias']\n        strict = False\n\n    model.load_state_dict(state_dict, strict=strict)\n\n\n\"\"\"\n    Load state dictionary of the pretrained model. \n\n\"\"\"\n\n\ndef load_state_dict(checkpoint_path, use_ema=False):\n    if checkpoint_path and os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        state_dict_key = 'state_dict'\n        if isinstance(checkpoint, dict):\n            if use_ema and 'state_dict_ema' in checkpoint:\n                state_dict_key = 'state_dict_ema'\n        if state_dict_key and state_dict_key in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[state_dict_key].items():\n                # strip `module.` prefix\n                name = k[7:] if k.startswith('module') else k\n                new_state_dict[name] = v\n            state_dict = new_state_dict\n        else:\n            state_dict = checkpoint\n        _logger.info(\"Loaded {} from checkpoint '{}'\".format(state_dict_key, checkpoint_path))\n        return state_dict\n    else:\n        _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n        raise FileNotFoundError()\n\n        \n  \n\"\"\"\n\n    @@@@ From model_loaders.py\n\n\"\"\"\n\ndef _conv_filter(state_dict, patch_size=16):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    out_dict = {}\n    for k, v in state_dict.items():\n        if 'patch_embed.proj.weight' in k:\n            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n        out_dict[k] = v\n    return out_dict\n\n\ndef vit_small_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=8, num_heads=6, mlp_ratio=3, qkv_bias=False,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=8, num_heads=6, mlp_ratio=3, qkv_bias=False,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\n    model.default_cfg = default_config['vit_small_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n    return model\n\n\ndef vit_base_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = default_config['vit_base_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n    return model\n\n\ndef vit_base_patch16_224_dino(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      dino=True, **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), dino=True, **kwargs)\n\n    model.default_cfg = default_config['vit_base_patch16_224_dino']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter, dino=True)\n    return model\n\n\ndef vit_base_patch16_224_moco(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\n    model.default_cfg = default_config['vit_base_patch16_224_moco']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter, moco=True)\n    return model\n\n\ndef vit_mae_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                      mae=True, **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), global_pooling=True, **kwargs)\n    model.default_cfg = default_config['vit_mae_patch16_224']\n    if pretrained:\n        load_pretrained(\n            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter, mae=True)\n    return model\n\n\ndef vit_large_patch16_224(pretrained=False, w_rel=False, **kwargs):\n    if w_rel:\n        model = VisionTransformerWRel(patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n                                      **kwargs)\n    else:\n        model = VisionTransformer(patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = default_config['vit_large_patch16_224']\n    if pretrained:\n        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n    return model\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Input arguments - input_arguments.py & Basic plotting utilities**","metadata":{}},{"cell_type":"code","source":"import argparse\n\n\ndef get_arg_parser():\n    parser = argparse.ArgumentParser(description='Training multi-class classifier')\n    parser.add_argument('--arc', type=str, default='vgg', metavar='N',\n                        help='Model architecture')\n    parser.add_argument('--train_dataset', type=str, default='imagenet', metavar='N',\n                        help='Testing Dataset')\n    parser.add_argument('--method', type=str,\n                        default='ours_c',\n                        choices=['rollout', 'lrp', 'partial_lrp', 'transformer_attribution', 'attn_last_layer',\n                                 'attn_gradcam', 'generic_attribution', 'ours', 'ours_c'],\n                        help='')\n    parser.add_argument('--thr', type=float, default=0.,\n                        help='threshold')\n    parser.add_argument('--start_layer', type=int, default=4,\n                        help='start_layer')\n    parser.add_argument('--K', type=int, default=1,\n                        help='new - top K results')\n    parser.add_argument('--save-img', action='store_true',\n                        default=True,\n                        help='')\n    parser.add_argument('--no-ia', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-fx', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-fgx', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-m', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--no-reg', action='store_true',\n                        default=False,\n                        help='')\n    parser.add_argument('--is-ablation', type=bool,\n                        default=False,\n                        help='')\n    parser.add_argument('--len-lim', type=int,\n                        default=1,\n                        help='')\n    parser.add_argument('--imagenet-seg-path', type=str, required=True)\n\n    return parser\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\n\n\ndef image_vizformat(img):\n    inr = transforms.Normalize(mean=[-0.5/.5, -0.5/.5, -0.5/.5], std=[1/0.5, 1/0.5, 1/0.5])\n    img = inr(img[0])\n    img = torch.permute(img, (1, 2, 0))\n    return img.detach().cpu().numpy()\n\ndef plot(image, isImage=True):\n    if isImage:\n        image = image_vizformat(image)\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.show()\n    \ndef plot_img_mask(image, mask):\n    fig, axs = plt.subplots(nrows=1, ncols=2)\n    axs[0].imshow(image_vizformat(image))\n    axs[0].axis(\"off\")\n    \n    axs[1].imshow(mask, cmap=\"gray\")\n    axs[1].axis(\"off\")\n    \n    plt.show()\n    \ndef plotside(i1, i2, cm1=\"autumn_r\", cm2=\"autumn_r\", title=\"Title\", save_path=None):\n    fig, axs = plt.subplots(nrows=1, ncols=2)\n    plt.title(title)\n    axs[0].imshow(i1, cmap=cm1)\n    axs[1].imshow(i2, cmap=cm2)\n    axs[0].axis(\"off\")\n    axs[1].axis(\"off\")\n    \n    if save_path is None:\n        plt.show()\n    else:\n        plt.savefig(save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=\"red\"> **Evaluation Metrics**</font>","metadata":{}},{"cell_type":"code","source":"def calculate_iou(pred_mask, true_mask):\n    # Ensure masks are binary (0 or 1)\n    pred_mask = np.where(pred_mask > 0.5, 1, 0)\n    true_mask = np.where(true_mask > 0.5, 1, 0)\n    \n    # Calculate Intersection and Union\n    intersection = np.logical_and(pred_mask, true_mask).sum()\n    union = np.logical_or(pred_mask, true_mask).sum()\n    \n    # Calculate IoU\n    iou = intersection / union if union != 0 else 0\n    \n    return round(iou, 2)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=\"#DC143C\"> **Building the Vision Transformer model**","metadata":{}},{"cell_type":"markdown","source":"## **Initializing the weight**","metadata":{}},{"cell_type":"code","source":"def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n                      \"The distribution of values may be incorrect.\",\n                      stacklevel=2)\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Patch Embeddings**","metadata":{}},{"cell_type":"code","source":"class PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **MLP**","metadata":{}},{"cell_type":"code","source":"class Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Attention**","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5  # This is root_over(dh) with which we divided Q.KT\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)  # Attention's multiplication. That is, we have to pass the attention output through a Linear layer. That is the linear layer.\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.attn_gradients = None\n        self.attention_map = None\n\n        self.vproj = None\n        self.input = None\n\n    def save_input(self, z):\n        self.input = z\n\n    def get_input(self):\n        return self.input\n\n    # Backward hook. Therefore, param is automatically gradients for that layer.\n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n\n    def get_attn_gradients(self):\n        return self.attn_gradients\n\n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n\n    def get_attention_map(self):\n        return self.attention_map\n\n    def forward(self, x, register_hook=False):\n        b, n, _, h = *x.shape, self.num_heads\n        self.save_input(x)\n        \n        print(\"\\n\\n\\n ================= Inside attention block ================ \\n\\n\")\n        \n        qkv = self.qkv(x)\n        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n        \n        print(f\"qkv shape: {qkv.shape}\\n\\t q: {q.shape}\\n\\t k: {k.shape}\\n\\t v:{v.shape}\\n\\tweight of qkv: {self.qkv.weight.shape}\")\n        \n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n        \n        attn = dots.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n    \n        out = torch.einsum('bhij,bhjd->bhid', attn, v)  # Simple torch.matmul actually.\n        \n        print(f\"dots : {dots.shape}\")\n        print(f\"attn i.e. softmax of dots: {attn.shape}\")\n        print(f\"out shape: {out.shape}  | attn * v -> bhij,bhjd->bhid\")\n\n        self.save_attention_map(attn)\n        if register_hook:\n            attn.register_hook(self.save_attn_gradients)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        \n        print(f\"out after rearrange: {out.shape} | b h n d -> b n (h d)\")\n\n        out = self.proj(out)\n        out = self.proj_drop(out)\n        \n\n        self.vproj = torch.matmul(rearrange(v, 'b h n d -> b n (h d)'), self.proj.weight.t())  # v is multipled with proj weight even for the attention output. We can see in out=self.proj(out) line.\n        # The thing is that, v was not previously multiplied with proj. instead we multipled the whole result. The paper says v is multipled with proj. Therefore, it is done here.\n\n        print(f\"out after proj: {out.shape} | W_proj shape: {self.proj.weight.shape}\")\n        print(f\"vproj after proj: {self.vproj.shape}\\n\")\n        print(\"================ End of attn block===============\\n\")\n        \n        \n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Encoder block**","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, block_index=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n        self.norm2 = norm_layer(dim)\n\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)  # hidden_features is the output layer's neuron count.\n\n        self.input = None\n        self.output = None\n        self.tilde = None\n        \n        self.block_index = block_index\n\n    def save_input(self, z):\n        self.input = z\n\n    def get_input(self):\n        return self.input\n\n    def save_tilde(self, z):\n        self.tilde = z\n\n    def get_tilde(self):\n        return self.tilde\n\n    def save_output(self, z):\n        self.output = z\n\n    def get_output(self):\n        return self.output\n\n    def forward(self, x, register_hook=False):\n        print(\"\\n\\n\\n___________________________________________________________________\\n\")\n        print(f\"Log for block: {self.block_index}\\n\\n\")\n        \n        self.save_input(x)\n        print(f\"shape of x: {x.shape}\")\n        \n        attn_out = self.attn(self.norm1(x), register_hook=register_hook)\n        out1 = x + attn_out\n        self.save_tilde(out1)\n        \n        \n        mlp_out = self.mlp(self.norm2(out1))\n        out = out1 + mlp_out\n        self.save_output(out)\n        \n        print(f\"attn_out: {attn_out.shape} | out1 = x + attn_out: {out1.shape}\")\n        print(f\"mlp_out: {mlp_out.shape} | out = out1 + mlp_out: {out.shape}\")\n        \n\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#A02334\"> **Vision Transformer Model** </font>","metadata":{}},{"cell_type":"code","source":"\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, drop_rate=0., attn_drop_rate=0., norm_layer=nn.LayerNorm,\n                 global_pooling=False, dino=False):\n        super().__init__()\n        self.global_pooling = global_pooling\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, norm_layer=norm_layer, block_index=i)\n            for i in range(depth)])\n        if global_pooling:\n            self.fc_norm = norm_layer(embed_dim)\n        else:\n            self.norm = norm_layer(embed_dim)\n\n        # Classifier head\n        if dino:\n            self.head = nn.Linear(embed_dim * 2, num_classes) if num_classes > 0 else nn.Identity()\n        else:\n            self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        self.cls_gradients = None\n        self.input_grad = None\n        self.dino = dino\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def save_cls_gradients(self, cls_gradients):\n        self.cls_gradients = cls_gradients\n\n    def get_cls_gradients(self):\n        return self.cls_gradients\n\n    def save_input_gradients(self, input_grad):\n        self.input_grad = input_grad\n\n    def get_input_gradients(self):\n        return self.input_grad\n\n    def forward(self, x, register_hook=False):\n        B = x.shape[0]\n        #         if register_hook:\n        #             x.register_hook(self.save_input_gradients)\n\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.blocks[:-1]:\n            x = blk(x, register_hook=register_hook)\n\n        x.register_hook(self.save_cls_gradients)\n\n        x = self.blocks[-1](x, register_hook=register_hook)\n\n        if self.global_pooling:\n            x = x[:, 1:, :].mean(axis=1)\n            x = self.fc_norm(x)\n        elif self.dino:\n            x = self.norm(x)\n            x = torch.cat((x[:, 0].unsqueeze(-1), x[:, 1:, :].mean(axis=1).unsqueeze(-1)), dim=-1)\n            x = x.reshape(x.shape[0], -1)\n        else:\n            x = self.norm(x)\n            x = x[:, 0]\n\n        x = self.head(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <br><br><center><font size=\"+6\"> **Run and Test** </font><br><br>","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/imagenet-seg-5000/gtsegs_ijcv.mat\"\ndata_length = 2\nbatch_size = 1\nnum_workers = 7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n])\nlabel_transform = transforms.Compose([transforms.Resize((224, 224), Image.NEAREST), ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = ImagenetSegLoader(data_path, data_length, transform=image_transform, target_transform=label_transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\ndataloader = tqdm(dataloader)  # Would help tracking loop iteration along with setting some verbose text.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, masks = [], []\n\nfor img, mask in dataloader:\n    images.append(img.to(device))\n    masks.append(mask.to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = vit_base_patch16_224_dino(pretrained=True).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model have 12 blocks and 12 heads. As 12 blocks i.e. 12 layers, therefore, attention is called 12 times and hence output printed for 12 times.","metadata":{}},{"cell_type":"code","source":"idx = 0\nx = images[idx]\noutput = model(x, register_hook=True)\n\n# clear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title - Gradient Visualization\n\n# index = np.argmax(output.cpu().data.numpy(), axis=-1)\n# out_sum = torch.sum(output)\n\n# one_hot = np.zeros((batch_size, output.size()[-1]), dtype=np.float32)\n# one_hot[np.arange(batch_size), index] = 1\n# one_hot = torch.from_numpy(one_hot).requires_grad_(True).to(device)\n# one_hot = torch.sum(one_hot)\n\n# def grad_accumulator():\n#     gradients = torch.eye(197, 197)\n#     for blk in model.blocks:\n#         grd = blk.attn.get_attn_gradients()\n#         grd = grd.reshape(-1, grd.shape[-2], grd.shape[-1]).mean(0)\n#         grd = torch.maximum(torch.zeros_like(grd), grd)  # Only positive values\n#         gradients = torch.mm(gradients, grd) + gradients\n#     return gradients, grd # Last layer's gradient is in grd\n\n\n# def grad_hmp(grd):\n#     grad_map = grd[1:, 1:].clamp(min=0).mean(0).reshape(14, 14).detach().cpu().numpy()\n#     grad_map = cv2.resize(grad_map, mask.shape, interpolation = cv2.INTER_NEAREST)\n#     thres_val = grad_map.mean()\n#     _, grad_map = cv2.threshold(grad_map, thres_val, 255, cv2.THRESH_BINARY)\n#     plt.imshow(grad_map, cmap=\"autumn_r\")\n#     plt.title(\"Accumulated attention gradients\")\n#     plt.colorbar()\n#     plt.axis(\"off\")\n\n\"\"\"\n    Calculating Positive Gradients\n\"\"\"\n\n# model.zero_grad()\n# one_hot.backward(retain_graph=True)\n# pos_grads = grad_accumulator() \n# grad_hmp(pos_grads)\n\n\"\"\"\n    \n    Calculating negative gradients\n\n\"\"\"\n\n# model.zero_grad()\n\n# # Sum of non-class probs = 1 - current_onehot as curr_onehot = softmax(output[max_idx])\n# one_hot = out_sum - one_hot\n# one_hot.backward(retain_graph=True)\n\n# neg_gradients = grad_accumulator()\n# grad_hmp(neg_gradients)\n\n\n\"\"\"\n    Difference between positive and negative hmp\n\"\"\"\n\n# diff_grad_map = grad_map - ngrad_map\n# plt.imshow(diff_grad_map, cmap=\"autumn_r\")\n# plt.colorbar()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Beyond Intuition tokenwise method**","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n\ndef beyond_intuition_tokenwise(model, x, device, onehot=None, index=None, steps=20, start_layer=1, samples=20, noise=0.1, mae=False, ssl=False, dino=False, taken_head_idx = None):\n\n    # A dictionary to track down all the output at every step, so we can visualize it.\n    stat_dict = {\n        \"input\": x,\n        \"steps\": steps,\n        \"start_layer\": start_layer,\n    }\n\n    b = x.shape[0]  # Batch size\n    x = x.to(device)\n\n    output = model(x)\n    \n    index = np.argmax(output.cpu().data.numpy(), axis=-1)\n    out_sum = torch.sum(output)\n\n    one_hot = np.zeros((batch_size, output.size()[-1]), dtype=np.float32)\n    one_hot[np.arange(batch_size), index] = 1\n    one_hot = torch.from_numpy(one_hot).requires_grad_(True).to(device)\n    one_hot = torch.sum(one_hot)\n\n    model.zero_grad()\n    one_hot.backward(retain_graph=True)\n\n    _, num_head, num_tokens, _ = model.blocks[-1].attn.get_attention_map().shape\n    R = torch.eye(num_tokens, num_tokens).expand(b, num_tokens, num_tokens).to(device)\n    \n    stat_dict['final_blk_attn_shape'] = model.blocks[-1].attn.get_attention_map().shape\n    stat_dict['pre_attn_perception_R'] = R\n    stat_dict['attention_perception'] = dict()\n\n\n    for blk_idx, blk in enumerate(model.blocks):\n        if blk_idx < start_layer - 1:\n            continue\n\n        # Calculate alpha in the paper.\n        z = blk.get_input()  # We can even call it z.\n        vproj = blk.attn.vproj  # vproj = Z * Wv * W(l).\n        order = torch.linalg.norm(vproj, dim=-1).squeeze() / torch.linalg.norm(z, dim=-1).squeeze()  # Z * Wv * W(l) / Z    --->  V / Z.  # Shape --> (197,)\n        m = torch.diag_embed(order)  # Converted the order into a diagonal matrix. # Shape (197, 197)\n\n        # Get attention map A_i which will be multiplied by alpha_i.\n        cam = blk.attn.get_attention_map()  # Shape --> (batch_size, num_heads, num_tokens, num_tokens)   (12, 197, 197)\n        cam = cam.reshape(-1, cam.shape[-2], cam.shape[-1])  # Make the shape (num_heads, num_tokens, num_tokens).  \n\n        \n        if taken_head_idx is None:\n            cam = cam.mean(0)  # Then take headwise mean and make it of shape (num_tokens, num_tokens).\n            # That is, take average of all head's attention for a token.\n        else:\n            cam = cam[taken_head_idx]  # Select specific head only.\n\n        # Also, try taking variance of gradient of attention and multiply then with that.\n        O_t = torch.matmul(cam.to(device), m.to(device))  # O = AZW   >>>  cam shape (197, 197) |  m shape (197, 197)\n        R = R + torch.matmul(O_t, R.to(device))\n\n        \n        stat_dict['attention_perception'][f\"rOps_{blk_idx}\"] = {\n            \"z\": z,\n            \"m\": m,\n            \"order\": order,\n            \"vproj\": vproj,\n            \"cam_noreshape\": cam,\n            \"mean_cam\": cam.mean(0),  # Headwise mean\n            \"O\": O_t,\n            \"R\": R,\n        }\n\n\n    if ssl:\n        if mae:\n            return R[:, 1:, 1:].abs().mean(axis=1)\n        elif dino:\n            return R[:, 1:, 1:].abs().mean(axis=1) + R[:, 0, 1:].abs()\n        else:\n            return R[:, 0, 1:].abs()\n\n    stat_dict[\"R_attn_perception\"] = R\n    stat_dict['reasoning_feedback'] = dict()\n\n    total_gradients = torch.zeros(b, num_head, num_tokens, num_tokens, device=device)\n    for alpha in np.linspace(0, 1, steps):\n        data_scaled = x * alpha\n        output = model(data_scaled, register_hook=True)\n        one_hot = np.zeros((b, output.size()[-1]), dtype=np.float32)\n        one_hot[np.arange(b), index] = 1\n        one_hot = torch.from_numpy(one_hot).requires_grad_(True).to(device)\n        one_hot = torch.sum(one_hot * output)\n\n        model.zero_grad()\n        one_hot.backward(retain_graph=True)\n\n        gradients = model.blocks[-1].attn.get_attn_gradients()\n        total_gradients += gradients\n\n        stat_dict['reasoning_feedback'][f\"alpha_{alpha}\"] = {\n            \"data_scaled\": data_scaled,\n            \"output\": output,\n            \"one_hot\": one_hot,\n            \"gradients\": gradients,\n            \"gradient_shape\":  model.blocks[-1].attn.get_attn_gradients().shape,\n            \"total_gradients\": total_gradients,\n        }\n\n    W_state = (total_gradients / steps).clamp(min=0).mean(1).reshape(b, num_tokens, num_tokens)\n    R = W_state * R.abs()\n\n    stat_dict[\"W_state\"] = W_state\n    stat_dict[\"R_resoaning_feedback\"] = R\n\n\n    if mae:\n        R_MAE = R[:, 1:, 1:].mean(axis=1)\n        stat_dict[\"R\"] = (R_MAE, \"MAE\")\n        return R_MAE, stat_dict\n\n    elif dino:\n        R_DINO = (R[:, 1:, 1:].mean(axis=1) + R[:, 0, 1:])\n        stat_dict[\"R\"] = (R_DINO, \"DINO\")\n        return R_DINO, stat_dict\n\n    else:\n        R_general = R[:, 0, 1:]\n        stat_dict[\"R\"] = (R_general, \"General Case\")\n        return R_general, stat_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Attribution result vs Mask - Test for Tokenwise method's weakness**","metadata":{}},{"cell_type":"code","source":"def attr_vs_mask(range_lim=None):\n    os.makedirs(\"/kaggle/working/attn_vs_mask\", exist_ok=True)\n    for idx in tqdm(range(range_lim)):\n        x = images[idx]\n        heatmap, _ = beyond_intuition_tokenwise(model, x, device, dino=True, steps=30)\n        \n        # Heatmap processing\n        hmp = heatmap.reshape(14, 14).detach().cpu().numpy()\n        hmp = cv2.resize(hmp, (224, 224), interpolation = cv2.INTER_NEAREST)  # reshape to 224, 224\n\n        # Mask\n        mask = masks[idx]\n        mask = mask[0].detach().cpu()\n        \n        iou_score = calculate_iou(hmp, mask.detach().cpu())  # Calculate iou score.\n        plotside(hmp, mask, title=f\"IOU score: {iou_score}\", save_path=f\"attn_vs_mask/{idx}.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}