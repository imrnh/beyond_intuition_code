{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio, warnings, os\n",
    "\n",
    "from interpretation_methods import *\n",
    "from utils.imagenet_seg_loader import ImagenetSegLoader\n",
    "from utils.model_loaders import vit_base_patch16_224_dino, vit_base_patch16_224\n",
    "from utils.input_arguments import get_arg_parser\n",
    "from utils.saver import Saver\n",
    "from utils.sideplot import side_plot\n",
    "from utils.image_denorm import image_vizformat\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.switch_backend(\"agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"lib/dataset/gtsegs_ijcv.mat\"\n",
    "data_length = 3\n",
    "batch_size = 1\n",
    "num_workers = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "label_transform = transforms.Compose([transforms.Resize((224, 224), Image.NEAREST), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImagenetSegLoader(data_path, data_length, transform=image_transform, target_transform=label_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "dataloader = tqdm(dataloader)  # Would help tracking loop iteration along with setting some verbose text.\n",
    "\n",
    "# model = vit_base_patch16_224_dino(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_vizformat(img):\n",
    "    inr = transforms.Normalize(mean=[-0.5/.5, -0.5/.5, -0.5/.5], std=[1/0.5, 1/0.5, 1/0.5])\n",
    "    img = inr(img[0])\n",
    "    img = torch.permute(img, (1, 2, 0))\n",
    "    return img.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "masks = []\n",
    "\n",
    "for ix, d in tqdm(enumerate(dataloader)):\n",
    "    imgs.append(d[0])\n",
    "    masks.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdpath = \"C:/Users/muimr/Research/Vit Interpret/Codes/beyond_intuition/lib/benchmark__trained_on_noisy_data/ff.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils.load_pretrained import load_pretrained\n",
    "from utils.model_loaders import _conv_filter\n",
    "from vision_transformer.vit import VisionTransformer\n",
    "from utils.config import default_config\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def vit_base_patch16_224(pretrained=False, url_given=None, **kwargs):\n",
    "    \n",
    "    model = VisionTransformer(patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "                                  norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    \n",
    "    if url_given is not None:\n",
    "        default_config['vit_base_patch16_224']['url'] = url_given\n",
    "    \n",
    "    model.default_cfg = default_config['vit_base_patch16_224']\n",
    "    if pretrained:\n",
    "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config['vit_base_patch16_224']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_base_patch16_224(pretrained=True, \n",
    "                             url_given=\"C:/Users/muimr/Research/Vit Interpret/Codes/beyond_intuition/lib/pretrained_model/jx_vit_base_p16_224-80ecf9dd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), mdpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = vit_base_patch16_224(pretrained=True, url_given=mdpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1keys = list((torch.load(\"C:/Users/muimr/Research/Vit Interpret/Codes/beyond_intuition/lib/pretrained_model/jx_vit_base_p16_224-80ecf9dd.pth\")).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2keys = list(torch.load(mdpath).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = model2(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
