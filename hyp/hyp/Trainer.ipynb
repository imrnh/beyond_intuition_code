{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e089d-e9f1-4735-a223-f33b7c5ec08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ~/.kaggle\n",
    "# !mv kaggle.json ~/.kaggle/kaggle.json\n",
    "# !pip3 install kaggle\n",
    "# !kaggle datasets download -d sautkin/imagenet1k0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a563ed-6ce6-4e24-8227-43a23170e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install unzip\n",
    "# # !mkdir imagenet1k0/\n",
    "# !unzip -q imagenet1k0.zip -d imagenet1k0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74c942-b4bf-4193-9e7c-a2291465c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 00220 folders\n",
    "# !rm -rf imagenet1k0/00220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c0dd45-df5b-4cd8-98dd-324c89a613ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir(\"dataset/train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18872fac-f6fa-467f-a183-1325eb54bfde",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16be252c-8c22-49e3-a97b-a8be7799f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py\n",
      "  Downloading h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from h5py) (1.24.4)\n",
      "Downloading h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, h5py, einops\n",
      "Successfully installed einops-0.8.0 h5py-3.11.0 tqdm-4.66.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install h5py einops tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd78ce4-a92b-4e95-9599-6261a45f758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810467d1-de3e-42ab-828f-570d692a1ad1",
   "metadata": {},
   "source": [
    "# **Preparing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158dae05-3c60-4488-81b6-f93f1fe444fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"imagenet1k0\"\n",
    "out_p = \"dataset\"\n",
    "\n",
    "\n",
    "def make_subset(subset, out_path, file_count, start_idx):\n",
    "    sub_dirs = os.listdir(input_path)\n",
    "    \n",
    "    # make folders\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    os.makedirs(out_path + f\"/{subset}/\", exist_ok=True) \n",
    "    \n",
    "    for sdir in tqdm(sub_dirs):\n",
    "        files = os.listdir(f\"{input_path}/{sdir}\")\n",
    "        os.makedirs(out_path  + f\"/{subset}/{sdir}/\", exist_ok=True)        \n",
    "        \n",
    "        total_fc = len(files)\n",
    "        \n",
    "        end_idx = None\n",
    "        if (start_idx + file_count) <= total_fc:\n",
    "            end_idx = (start_idx + file_count)\n",
    "        \n",
    "        for fp in files[start_idx: end_idx]:\n",
    "            src_path = input_path + f\"/{sdir}/{fp}\"\n",
    "            dest_path = out_path  + f\"/{subset}/{sdir}/{fp}\"\n",
    "            \n",
    "            shutil.copyfile(src_path, dest_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15205b-5567-4622-8c9b-95f45a17f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_subset(\"train\", out_p, 300, 0) # Make train\n",
    "make_subset(\"mini_val\", out_p, 30, 300) # Make test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2c79e3-a4a0-4b92-96d8-37478e64fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# dir_root = \"dataset/mini_val/\"\n",
    "\n",
    "# for x in range(100, 220):\n",
    "#     directory = dir_root + f\"00{str(x)}\"\n",
    "#     shutil.rmtree(directory)\n",
    "#     print(directory)\n",
    "\n",
    "shutil.rmtree(\"dataset/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a87d6d-8c6c-4762-8a2c-4664885eb76c",
   "metadata": {},
   "source": [
    "# **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12051b0c-fd6c-44e7-8f21-ecd97b8b8181",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vit_small_patch16_224\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Train the models for given saliency map.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from utils.model_loaders import vit_small_patch16_224\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, data_path, epochs, batch_size, num_workers, lr, weight_decay) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        self.save_dir = \"custom_trained_models/\"\n",
    "        self.data_path = data_path\n",
    "        self.train_logs = []\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Model, Optimizer and Loss function setup\n",
    "        self.model = vit_small_patch16_224(pretrained=False).cuda()\n",
    "\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=self.lr, weight_decay=weight_decay)\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "\n",
    "        # Data loading and transforms\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "\n",
    "        self.dataset = datasets.ImageFolder(root=self.data_path + \"/train\", transform=self.image_transform)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, drop_last=False)\n",
    "\n",
    "        self.validation_dataset = datasets.ImageFolder(root=self.data_path + \"/mini_val\", transform=self.image_transform)\n",
    "        self.validation_dataloader = DataLoader(self.validation_dataset, batch_size=int(self.batch_size / 2), shuffle=False, num_workers=self.num_workers, drop_last=False)\n",
    "    \n",
    "        print(f\"{len(self.dataloader) * batch_size } Train Images found\")\n",
    "        print(f\"{len(self.validation_dataloader) * batch_size} Validation Images found\")\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "        Train the model and finally save it.\n",
    "    \"\"\"\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            val_accuracy = 0.0\n",
    "            \n",
    "            # Training\n",
    "            for batch in tqdm(self.dataloader):\n",
    "                x, y = batch\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "                y_hat = self.model(x)\n",
    "                loss = self.criterion(y_hat, y)\n",
    "                train_loss += loss.detach().cpu().item() / len(self.dataloader)\n",
    "\n",
    "                loss.backward()  # backprop calculation\n",
    "                self.optimizer.step()  # Updating the weight based on these calculation.\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # tqdm._instances.clear()\n",
    "\n",
    "            # Validation\n",
    "            for batch in tqdm(self.validation_dataloader):\n",
    "                xv, yv = batch\n",
    "                xv, yv = xv.cuda(), yv.cuda()\n",
    "                yv_hat = self.model(xv)\n",
    "                vloss = self.criterion(yv_hat, yv)\n",
    "                \n",
    "                val_loss += vloss.detach().cpu().item() / len(self.validation_dataloader)\n",
    "                val_accuracy += self.measure_accuracy(yv_hat, yv) / len(self.validation_dataloader)\n",
    "\n",
    "                # tqdm._instances.clear()\n",
    "\n",
    "                \n",
    "            self.callbacks(train_loss, val_loss, val_accuracy, epoch)\n",
    "\n",
    "        \n",
    "    def save_model(self, model_name):\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        model_path = os.path.join(self.save_dir, f\"jx_vit_base_p16_224_{model_name}.pth\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "        \n",
    "    def embd_to_class(self, batched_embed):\n",
    "        class_indices = []\n",
    "        for embd in batched_embed:\n",
    "            cls_idx = torch.argmax(embd)\n",
    "            class_indices.append(cls_idx)\n",
    "        return torch.Tensor(class_indices)\n",
    "    \n",
    "    def measure_accuracy(self, yhat, y):\n",
    "        yhat_idx = self.embd_to_class(yhat)\n",
    "\n",
    "        accuracy = 0.0\n",
    "        for yh, yval in zip(yhat_idx, y):\n",
    "            accuracy += 1 if (yh == yval) else 0\n",
    "            \n",
    "        return accuracy\n",
    "    \n",
    "\n",
    "    def callbacks(self, t_loss, v_loss, vacc, epoch):\n",
    "        self.train_logs.append({'train_loss': t_loss, 'val_loss': v_loss, 'val_accuracy': vacc})\n",
    "        print(f\"\"\"tr_loss: {t_loss:.5f} \\t val_loss: {v_loss:.5f} \\t val_acc: {vacc:.5f}\"\"\")\n",
    "            \n",
    "        # Save best model per epoch\n",
    "        if epoch > 1:\n",
    "            prev_vl = self.train_logs[-2]['val_loss']\n",
    "            prev_vacc = self.train_logs[-2]['val_accuracy']\n",
    "\n",
    "            if vacc >= prev_vacc :\n",
    "                self.save_model(f\"raw_images_{str(epoch)}_max_validation_accuracy\")\n",
    "                print(f\"Saved for acc:{vacc}\")\n",
    "\n",
    "            if v_loss <= prev_vl :\n",
    "                self.save_model(f\"raw_images_{str(epoch)}_min_validation_loss\")\n",
    "                print(f\"Saved for loss: {v_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a2d0c4-4c56-4149-af78-90c0ee4bd654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30048 Train Images found\n",
      "6048 Validation Images found\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(data_path=\"dataset\", epochs=25, batch_size=96, num_workers=1, lr=0.001, weight_decay=0.00004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111598d2-307a-435e-8cc3-8654eb877c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edc2bd6-60bd-46cd-93d2-61e9bcacfe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f7550d-636d-4694-9edc-637cfcc0b161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"/kaggle/working/custom_trained_models\")) * 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3746cc54-87ba-4a22-9eec-e4055339dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "min_vloss_path = \"custom_trained_models/jx_vit_base_p16_224_raw_images_23_min_validation_loss.pth\"\n",
    "max_vacc_path = \"custom_trained_models/jx_vit_base_p16_224_raw_images_24_max_validation_accuracy.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815131d6-657f-4104-afff-8cfb0f5a9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = datasets.ImageFolder(root=\"dataset/mini_val\", transform=image_transform)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e08982-73db-4c7f-a988-b5313343117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "model = torch.load(min_vloss_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea1d4e4e-59f8-4097-9fdb-20c25276277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def embd_to_class(self, batched_embed):\n",
    "    class_indices = []\n",
    "    for embd in batched_embed:\n",
    "        cls_idx = torch.argmax(embd)\n",
    "        class_indices.append(cls_idx)\n",
    "    return torch.Tensor(class_indices)\n",
    "\n",
    "def measure_accuracy(self, yhat, y):\n",
    "    yhat_idx = self.embd_to_class(yhat)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    for yh, yval in zip(yhat_idx, y):\n",
    "        accuracy += 1 if (yh == yval) else 0\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0973acce-f2f9-4eb9-a5f1-6095206936db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      2\u001b[0m val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mvalidation_dataloader):\n\u001b[1;32m      5\u001b[0m     xv, yv \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      6\u001b[0m     xv, yv \u001b[38;5;241m=\u001b[39m xv\u001b[38;5;241m.\u001b[39mcuda(), yv\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "val_loss = 0.0\n",
    "val_accuracy = 0.0\n",
    "\n",
    "for batch in tqdm(self.validation_dataloader):\n",
    "    xv, yv = batch\n",
    "    xv, yv = xv.cuda(), yv.cuda()\n",
    "    yv_hat = model(xv)\n",
    "    vloss = criterion(yv_hat, yv)\n",
    "    \n",
    "    val_loss += vloss.detach().cpu().item() / len(self.validation_dataloader)\n",
    "    val_accuracy += self.measure_accuracy(yv_hat, yv) / len(self.validation_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57798f-188d-4217-90c2-a486f5c2b70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
