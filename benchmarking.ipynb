{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import os, shutil, cv2\n",
    "from utils.model_loaders import vit_small_patch16_224, vit_base_patch16_224\n",
    "import plotly.express as ex\n",
    "from utils.image_denorm import image_vizformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4062095d164ae689f200eb9e3aadf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:47<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"lib/data/dataset_50/val\"\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(data_path, transform=image_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, num_workers=4, shuffle=False)\n",
    "dataloader = tqdm(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"lib/benchmarking_model/jx_vit_base_p16_224_raw_images_24_max_validation_accuracy.pth\"\n",
    "\n",
    "model = vit_base_patch16_224(pretrained=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embd_to_class(batched_embed):\n",
    "    class_indices = []\n",
    "    for embd in batched_embed:\n",
    "        cls_idx = torch.argmax(embd)\n",
    "        class_indices.append(cls_idx)\n",
    "    return torch.Tensor(class_indices)\n",
    "\n",
    "def measure_accuracy(yhat, y):\n",
    "    yhat_idx = embd_to_class(yhat)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    for yh, yval in zip(yhat_idx, y):\n",
    "        accuracy += 1 if (yh == yval) else 0\n",
    "        \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "val_loss = 0.0\n",
    "val_accuracy = 0.0\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    xv, yv = batch\n",
    "    xv, yv = xv.cuda(), yv.cuda()\n",
    "    yv_hat = model(xv)\n",
    "    vloss = criterion(yv_hat, yv)\n",
    "    \n",
    "    val_loss += vloss.detach().cpu().item() / len(dataloader)\n",
    "    val_accuracy += measure_accuracy(yv_hat, yv) / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
